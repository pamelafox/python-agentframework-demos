"""Inflado de contexto sin subagentes.

Cuando un agente usa herramientas que regresan salidas grandes (contenido de
archivos, resultados de bÃºsqueda, filas de base de datos), toda esa data cruda
se acumula en la ventana de contexto del agente. Tras mÃºltiples llamadas a
herramientas, esto infla el contexto, aumenta el costo y puede degradar el
rendimiento.

Este ejemplo demuestra el problema: un solo agente lee y busca archivos fuente
directamente. Cada lÃ­nea de cÃ³digo fuente fluye hacia el contexto del agente
junto con la conversaciÃ³n.

Diagrama:

 agent.run("pregunta del usuario")
 â”‚
 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚           Agente de InvestigaciÃ³n de CÃ³digo      â”‚
 â”‚                                                  â”‚
 â”‚  1. Llama list_project_files() â†’ lista completa  â”‚
 â”‚  2. Llama read_project_file() â†’ contenido        â”‚
 â”‚     completo agregado al contexto (N veces)      â”‚
 â”‚  3. Llama search_project_files() â†’ todas las     â”‚
 â”‚     lÃ­neas coincidentes agregadas al contexto    â”‚
 â”‚  4. Genera respuesta desde un contexto inflado   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â–¼
 respuesta (el agente vio TODO el contenido crudo)

Compara con agent_with_subagent.py para ver cÃ³mo los subagentes solucionan esto.
"""

import asyncio
import glob
import logging
import os
import sys
from typing import Annotated

from agent_framework import Agent, tool
from agent_framework.openai import OpenAIChatClient
from azure.identity.aio import DefaultAzureCredential, get_bearer_token_provider
from dotenv import load_dotenv
from pydantic import Field
from rich import print
from rich.logging import RichHandler

# â”€â”€ Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
handler = RichHandler(show_path=False, rich_tracebacks=True, show_level=False)
logging.basicConfig(level=logging.WARNING, handlers=[handler], force=True, format="%(message)s")
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# â”€â”€ Cliente OpenAI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv(override=True)
API_HOST = os.getenv("API_HOST", "github")

async_credential = None
if API_HOST == "azure":
    async_credential = DefaultAzureCredential()
    token_provider = get_bearer_token_provider(async_credential, "https://cognitiveservices.azure.com/.default")
    client = OpenAIChatClient(
        base_url=f"{os.environ['AZURE_OPENAI_ENDPOINT']}/openai/v1/",
        api_key=token_provider,
        model_id=os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT"],
    )
elif API_HOST == "github":
    client = OpenAIChatClient(
        base_url="https://models.github.ai/inference",
        api_key=os.environ["GITHUB_TOKEN"],
        model_id=os.getenv("GITHUB_MODEL", "openai/gpt-4o"),
    )
else:
    client = OpenAIChatClient(api_key=os.environ["OPENAI_API_KEY"], model_id=os.environ.get("OPENAI_MODEL", "gpt-4o"))

# â”€â”€ RaÃ­z del proyecto para herramientas de archivos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROJECT_DIR = os.path.join(os.path.dirname(__file__))


# â”€â”€ Herramientas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@tool
def list_project_files(
    directory: Annotated[str, Field(description="Relative directory path within the examples folder, e.g. '.' or 'spanish'.")],
) -> str:
    """Lista todos los archivos en el directorio indicado dentro de la carpeta examples."""
    logger.info("[ðŸ“‚ Herramienta] list_project_files('%s')", directory)
    target = os.path.join(PROJECT_DIR, directory)
    if not os.path.isdir(target):
        return f"Error: no se encontrÃ³ el directorio '{directory}'."
    entries = sorted(os.listdir(target))
    return "\n".join(entries)


@tool
def read_project_file(
    filepath: Annotated[str, Field(description="Relative file path within the examples folder, e.g. 'agent_middleware.py'.")],
) -> str:
    """Lee y devuelve el contenido completo de un archivo dentro de la carpeta examples."""
    logger.info("[ðŸ“„ Herramienta] read_project_file('%s')", filepath)
    target = os.path.join(PROJECT_DIR, filepath)
    if not os.path.isfile(target):
        return f"Error: no se encontrÃ³ el archivo '{filepath}'."
    with open(target) as f:
        return f.read()


@tool
def search_project_files(
    query: Annotated[str, Field(description="Text to search for (case-insensitive) across all .py files in the examples folder.")],
) -> str:
    """Busca en todos los .py dentro de examples las lÃ­neas que contengan el texto."""
    logger.info("[ðŸ” Herramienta] search_project_files('%s')", query)
    query_lower = query.lower()
    results: list[str] = []
    for fpath in sorted(glob.glob(os.path.join(PROJECT_DIR, "*.py"))):
        relpath = os.path.relpath(fpath, PROJECT_DIR)
        with open(fpath) as f:
            for lineno, line in enumerate(f, 1):
                if query_lower in line.lower():
                    results.append(f"{relpath}:{lineno}: {line.rstrip()}")
    if not results:
        return f"No se encontraron coincidencias para '{query}'."
    if len(results) > 50:
        return "\n".join(results[:50]) + f"\n... ({len(results) - 50} coincidencias mÃ¡s truncadas)"
    return "\n".join(results)


# â”€â”€ Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

agent = Agent(
    name="coding-assistant",
    client=client,
    instructions=(
        "Eres un asistente de programaciÃ³n Ãºtil. Respondes preguntas sobre bases de cÃ³digo, "
        "explicas patrones y ayudas a devs a entender el cÃ³digo. Usa las herramientas disponibles "
        "para listar, leer y buscar archivos Python del proyecto, y luego da una respuesta clara "
        "y bien organizada."
    ),
    tools=[list_project_files, read_project_file, search_project_files],
)

# â”€â”€ Query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

USER_QUERY = "Â¿QuÃ© patrones distintos de middleware se usan en este proyecto? Lee los archivos relevantes para averiguarlo."

async def main() -> None:
    """Ejecuta una consulta y registra el uso de tokens."""
    print("\n[bold]=== InvestigaciÃ³n de CÃ³digo SIN Subagentes ===[/bold]")
    print("[dim]Todo el contenido de archivos fluye directo a la ventana de contexto del agente.[/dim]\n")

    print(f"[blue]Usuario:[/blue] {USER_QUERY}")
    response = await agent.run(USER_QUERY)
    print(f"[green]Asistente:[/green] {response.text}\n")

    # Uso de tokens
    usage = response.usage_details or {}
    input_t = usage.get("input_token_count", 0) or 0
    output_t = usage.get("output_token_count", 0) or 0
    total_t = usage.get("total_token_count", 0) or 0

    print("[bold]â”€â”€ Uso de tokens â”€â”€[/bold]")
    print(f"[yellow]  Tokens del asistente:[/yellow]  input={input_t:,}  output={output_t:,}  total={total_t:,}")
    print()
    print("[dim]Todo el contenido crudo de archivos quedÃ³ en la ventana de contexto del agente.[/dim]")
    print("[dim]Compara con agent_with_subagent.py para ver el aislamiento de contexto en acciÃ³n.[/dim]\n")

    if async_credential:
        await async_credential.close()


if __name__ == "__main__":
    if "--devui" in sys.argv:
        from agent_framework.devui import serve

        serve(entities=[agent], auto_open=True)
    else:
        asyncio.run(main())
