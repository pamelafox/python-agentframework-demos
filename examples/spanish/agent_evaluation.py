import asyncio
import json
import logging
import os
import tempfile
from typing import Annotated

from agent_framework import ChatAgent, tool
from agent_framework.openai import OpenAIChatClient
from azure.ai.evaluation import (
    AzureOpenAIModelConfiguration,
    IntentResolutionEvaluator,
    OpenAIModelConfiguration,
    ResponseCompletenessEvaluator,
    TaskAdherenceEvaluator,
    ToolCallAccuracyEvaluator,
    evaluate,
)
from azure.identity.aio import DefaultAzureCredential, get_bearer_token_provider
from dotenv import load_dotenv
from pydantic import Field
from rich import print
from rich.logging import RichHandler
from rich.panel import Panel
from rich.table import Table

handler = RichHandler(show_path=False, rich_tracebacks=True, show_level=False)
logging.basicConfig(level=logging.WARNING, handlers=[handler], force=True, format="%(message)s")
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

load_dotenv(override=True)
API_HOST = os.getenv("API_HOST", "github")

async_credential = None
if API_HOST == "azure":
    async_credential = DefaultAzureCredential()
    token_provider = get_bearer_token_provider(async_credential, "https://cognitiveservices.azure.com/.default")
    client = OpenAIChatClient(
        base_url=f"{os.environ['AZURE_OPENAI_ENDPOINT']}/openai/v1/",
        api_key=token_provider,
        model_id=os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT"],
    )
    eval_model_config = AzureOpenAIModelConfiguration(
        type="azure_openai",
        azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
        azure_deployment=os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT"],
    )
elif API_HOST == "github":
    client = OpenAIChatClient(
        base_url="https://models.github.ai/inference",
        api_key=os.environ["GITHUB_TOKEN"],
        model_id=os.getenv("GITHUB_MODEL", "openai/gpt-5-mini"),
    )
    eval_model_config = OpenAIModelConfiguration(
        type="openai",
        base_url="https://models.github.ai/inference",
        api_key=os.environ["GITHUB_TOKEN"],
        model="openai/gpt-5-mini",
    )
else:
    client = OpenAIChatClient(
        api_key=os.environ["OPENAI_API_KEY"], model_id=os.environ.get("OPENAI_MODEL", "gpt-5-mini")
    )
    eval_model_config = OpenAIModelConfiguration(
        type="openai",
        api_key=os.environ["OPENAI_API_KEY"],
        model=os.environ.get("OPENAI_MODEL", "gpt-5-mini"),
    )

# Opcional: Establece AZURE_AI_PROJECT en .env para registrar resultados en Azure AI Foundry.
# Ejemplo: https://tu-cuenta.services.ai.azure.com/api/projects/tu-proyecto
AZURE_AI_PROJECT = os.getenv("AZURE_AI_PROJECT")


@tool
def get_weather(
    city: Annotated[str, Field(description="The city to get the weather forecast for.")],
    date_range: Annotated[str, Field(description="Date range in format 'YYYY-MM-DD to YYYY-MM-DD'.")],
) -> dict:
    """Devuelve el pronóstico del clima para una ciudad durante un rango de fechas."""
    logger.info(f"Obteniendo el clima para {city} ({date_range})")
    return {
        "city": city,
        "date_range": date_range,
        "forecast": [
            {"date": "Día 1", "high_f": 65, "low_f": 52, "conditions": "Parcialmente nublado"},
            {"date": "Día 2", "high_f": 70, "low_f": 55, "conditions": "Soleado"},
            {"date": "Día 3", "high_f": 62, "low_f": 50, "conditions": "Lluvia ligera"},
        ],
    }


@tool
def search_flights(
    origin: Annotated[str, Field(description="Departure city or airport code.")],
    destination: Annotated[str, Field(description="Arrival city or airport code.")],
    departure_date: Annotated[str, Field(description="Departure date in YYYY-MM-DD format.")],
    return_date: Annotated[str, Field(description="Return date in YYYY-MM-DD format.")],
) -> list[dict]:
    """Busca vuelos de ida y vuelta y devuelve opciones con precios."""
    logger.info(f"Buscando vuelos {origin} -> {destination} ({departure_date} a {return_date})")
    return [
        {"airline": "SkyAir", "price_usd": 850, "duration": "14h 20m", "stops": 1},
        {"airline": "OceanWings", "price_usd": 720, "duration": "16h 45m", "stops": 2},
        {"airline": "DirectJet", "price_usd": 1100, "duration": "12h 30m", "stops": 0},
    ]


@tool
def search_hotels(
    city: Annotated[str, Field(description="The city to search hotels in.")],
    checkin: Annotated[str, Field(description="Check-in date in YYYY-MM-DD format.")],
    checkout: Annotated[str, Field(description="Check-out date in YYYY-MM-DD format.")],
    max_price_per_night: Annotated[int, Field(description="Maximum price per night in USD.")],
) -> list[dict]:
    """Busca hoteles dentro de un presupuesto por noche y devuelve opciones con calificaciones."""
    logger.info(f"Buscando hoteles en {city} ({checkin} a {checkout}, máx ${max_price_per_night}/noche)")
    return [
        {"name": "Budget Inn Tokyo", "price_per_night_usd": 80, "rating": 3.8, "neighborhood": "Asakusa"},
        {"name": "Sakura Hotel", "price_per_night_usd": 120, "rating": 4.2, "neighborhood": "Shinjuku"},
        {"name": "Tokyo Garden Suites", "price_per_night_usd": 200, "rating": 4.6, "neighborhood": "Ginza"},
    ]


@tool
def get_activities(
    city: Annotated[str, Field(description="The city to find activities in.")],
    interests: Annotated[list[str], Field(description="List of interests, e.g. ['hiking', 'museums'].")],
) -> list[dict]:
    """Devuelve sugerencias de actividades para una ciudad según los intereses del usuario."""
    logger.info(f"Obteniendo actividades en {city} para intereses: {interests}")
    activities = []
    if "hiking" in [i.lower() for i in interests]:
        activities.extend(
            [
                {"name": "Excursión al Monte Takao", "cost_usd": 15, "duration": "4-5 horas"},
                {"name": "Caminata por Kamakura", "cost_usd": 25, "duration": "3 horas"},
            ]
        )
    if "museums" in [i.lower() for i in interests]:
        activities.extend(
            [
                {"name": "Museo Nacional de Tokio", "cost_usd": 10, "duration": "2-3 horas"},
                {"name": "teamLab Borderless", "cost_usd": 30, "duration": "2 horas"},
            ]
        )
    if not activities:
        activities = [{"name": "Tour a pie por la ciudad", "cost_usd": 0, "duration": "3 horas"}]
    return activities


@tool
def estimate_budget(
    total_budget: Annotated[int, Field(description="Total trip budget in USD.")],
    num_days: Annotated[int, Field(description="Number of days for the trip.")],
) -> dict:
    """Proporciona un desglose de presupuesto recomendado para vuelos, hoteles, actividades y comida."""
    logger.info(f"Estimando presupuesto: ${total_budget} para {num_days} días")
    flight_pct = 0.40
    hotel_pct = 0.30
    activities_pct = 0.15
    food_pct = 0.15
    return {
        "total_budget_usd": total_budget,
        "flights_usd": int(total_budget * flight_pct),
        "hotels_usd": int(total_budget * hotel_pct),
        "hotels_per_night_usd": int(total_budget * hotel_pct / num_days),
        "activities_usd": int(total_budget * activities_pct),
        "food_usd": int(total_budget * food_pct),
        "food_per_day_usd": int(total_budget * food_pct / num_days),
    }


tools = [get_weather, search_flights, search_hotels, get_activities, estimate_budget]

tool_definitions = [t.to_json_schema_spec()["function"] for t in tools]

AGENT_INSTRUCTIONS = (
    "Eres un asistente de planificación de viajes. Ayuda a los usuarios a planificar viajes "
    "consultando el clima, buscando vuelos y hoteles dentro del presupuesto, y sugiriendo "
    "actividades según sus intereses. Siempre proporciona un itinerario completo con costos "
    "para cada componente y asegúrate de que el total no exceda el presupuesto del usuario. "
    "Incluye información del clima para ayudar con el equipaje."
)

agent = ChatAgent(
    name="travel-planner",
    chat_client=client,
    instructions=AGENT_INSTRUCTIONS,
    tools=tools,
)


def convert_to_evaluator_messages(messages) -> list[dict]:
    """Convierte ChatMessages del agent framework al esquema de mensajes de Azure AI Evaluation.

    Remapea tipos de contenido: function_call -> tool_call, function_result -> tool_result.
    Ver: https://learn.microsoft.com/azure/ai-foundry/how-to/develop/agent-evaluate-sdk#agent-message-schema
    """
    evaluator_messages = []
    for msg in messages:
        role = str(msg.role.value) if hasattr(msg.role, "value") else str(msg.role)
        content_items = []
        for c in msg.contents:
            if c.type == "function_call":
                content_items.append(
                    {
                        "type": "tool_call",
                        "tool_call_id": c.call_id,
                        "name": c.name,
                        "arguments": json.loads(c.arguments) if isinstance(c.arguments, str) else c.arguments,
                    }
                )
            elif c.type == "function_result":
                if c.call_id:
                    if content_items:
                        evaluator_messages.append({"role": role, "content": content_items})
                        content_items = []
                    evaluator_messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": c.call_id,
                            "content": [
                                {
                                    "type": "tool_result",
                                    "tool_result": c.result,
                                }
                            ],
                        }
                    )
                    continue
                content_items.append(
                    {
                        "type": "tool_result",
                        "tool_result": c.result,
                    }
                )
            elif c.type == "text" and c.text:
                content_items.append({"type": "text", "text": c.text})
        if content_items:
            evaluator_messages.append({"role": role, "content": content_items})
    return evaluator_messages


def display_evaluation_results(results: dict[str, dict]) -> None:
    """Muestra los resultados de evaluación en una tabla formateada usando rich."""
    table = Table(title="Resultados de Evaluación del Agente", show_lines=True)
    table.add_column("Evaluador", style="cyan", width=28)
    table.add_column("Puntaje", style="bold", justify="center", width=8)
    table.add_column("Resultado", justify="center", width=10)
    table.add_column("Razón", style="dim", width=70)

    for evaluator_name, result in results.items():
        score = str(result.get("score", "N/A"))
        pass_fail = result.get("result", "N/A")
        reason = result.get("reason", "N/A")

        if pass_fail == "pass":
            result_str = "[green]aprobado[/green]"
        elif pass_fail == "fail":
            result_str = "[red]reprobado[/red]"
        else:
            result_str = str(pass_fail)

        table.add_row(evaluator_name, score, result_str, reason)

    print()
    print(table)


async def main():
    query = (
        "Planifica un viaje de 3 días de Nueva York a Tokio el próximo mes "
        "con un presupuesto de $2000. Me gusta el senderismo y los museos."
    )

    logger.info("Ejecutando el agente planificador de viajes...")
    response = await agent.run(query)
    print(Panel(response.text, title="Respuesta del Agente", border_style="blue"))

    eval_query = [
        {"role": "system", "content": AGENT_INSTRUCTIONS},
        {"role": "user", "content": [{"type": "text", "text": query}]},
    ]
    eval_response = convert_to_evaluator_messages(response.messages)

    ground_truth = (
        "Un itinerario completo de 3 días a Tokio desde Nueva York incluyendo: opciones de vuelos de ida y vuelta "
        "con precios, recomendaciones de hoteles dentro del presupuesto por noche, actividades de senderismo "
        "(ej. Monte Takao), visitas a museos (ej. Museo Nacional de Tokio, teamLab Borderless), pronóstico del "
        "clima para las fechas del viaje, un desglose completo de costos mostrando un total menor a $2000, "
        "y sugerencias de equipaje basadas en el clima."
    )

    logger.info("Ejecutando evaluadores de agente...")

    evaluator_kwargs = {"model_config": eval_model_config, "is_reasoning_model": True}
    result_keys = {
        "IntentResolution": "intent_resolution",
        "ResponseCompleteness": "response_completeness",
        "TaskAdherence": "task_adherence",
        "ToolCallAccuracy": "tool_call_accuracy",
    }

    if AZURE_AI_PROJECT:
        logger.info(f"Registrando resultados de evaluación en Azure AI project: {AZURE_AI_PROJECT}")

        eval_data_row = {
            "query": eval_query,
            "response": eval_response,
            "response_text": response.text,
            "ground_truth": ground_truth,
            "tool_definitions": tool_definitions,
        }

        with tempfile.NamedTemporaryFile(mode="w", suffix=".jsonl", delete=False, encoding="utf-8") as f:
            f.write(json.dumps(eval_data_row) + "\n")
            eval_data_file = f.name

        try:
            eval_result = evaluate(
                data=eval_data_file,
                evaluation_name="travel-planner-agent-eval",
                evaluators={
                    "intent_resolution": IntentResolutionEvaluator(**evaluator_kwargs),
                    "response_completeness": ResponseCompletenessEvaluator(**evaluator_kwargs),
                    "task_adherence": TaskAdherenceEvaluator(**evaluator_kwargs),
                    "tool_call_accuracy": ToolCallAccuracyEvaluator(**evaluator_kwargs),
                },
                # ResponseCompletenessEvaluator espera texto plano, no una lista de mensajes,
                # así que usamos response_text y ground_truth explícitamente.
                # Los demás evaluadores se auto-mapean correctamente ya que las claves de datos coinciden.
                evaluator_config={
                    "response_completeness": {
                        "column_mapping": {
                            "response": "${data.response_text}",
                            "ground_truth": "${data.ground_truth}",
                        }
                    },
                },
                azure_ai_project=AZURE_AI_PROJECT,
            )

            # Parsear resultados de la salida del batch evaluate()
            evaluation_results = {}
            rows = eval_result.get("rows", [])
            row = rows[0] if rows else {}

            for display_name, key in result_keys.items():
                evaluation_results[display_name] = {
                    "score": row.get(f"outputs.{key}.{key}", "N/A"),
                    "result": row.get(f"outputs.{key}.{key}_result", "N/A"),
                    "reason": row.get(f"outputs.{key}.{key}_reason", "N/A"),
                }

            display_evaluation_results(evaluation_results)

            studio_url = eval_result.get("studio_url")
            if studio_url:
                print(f"\n[bold blue]Ver resultados en Azure AI Foundry:[/bold blue] {studio_url}")
        finally:
            os.unlink(eval_data_file)
    else:
        intent_evaluator = IntentResolutionEvaluator(**evaluator_kwargs)
        completeness_evaluator = ResponseCompletenessEvaluator(**evaluator_kwargs)
        adherence_evaluator = TaskAdherenceEvaluator(**evaluator_kwargs)
        tool_accuracy_evaluator = ToolCallAccuracyEvaluator(**evaluator_kwargs)

        intent_result = intent_evaluator(query=eval_query, response=eval_response, tool_definitions=tool_definitions)
        completeness_result = completeness_evaluator(response=response.text, ground_truth=ground_truth)
        adherence_result = adherence_evaluator(
            query=eval_query, response=eval_response, tool_definitions=tool_definitions
        )
        tool_accuracy_result = tool_accuracy_evaluator(
            query=eval_query, response=eval_response, tool_definitions=tool_definitions
        )

        evaluation_results = {}
        for name, result in [
            ("IntentResolution", intent_result),
            ("ResponseCompleteness", completeness_result),
            ("TaskAdherence", adherence_result),
            ("ToolCallAccuracy", tool_accuracy_result),
        ]:
            key = result_keys[name]
            evaluation_results[name] = {
                "score": result.get(key, "N/A"),
                "result": result.get(f"{key}_result", "N/A"),
                "reason": result.get(f"{key}_reason", result.get("error_message", "N/A")),
            }

        display_evaluation_results(evaluation_results)

    if async_credential:
        await async_credential.close()


if __name__ == "__main__":
    asyncio.run(main())
