"""Enrutador de mensajes de clientes con salidas estructuradas y aristas switch-case.

Demuestra: response_format= para salidas estructuradas confiables, @executor
para un nodo convertidor y add_switch_case_edge_group para enrutamiento mÃºltiple.

Un agente Clasificador usa un modelo Pydantic como response_format para que
la categorÃ­a siempre sea un valor vÃ¡lido y tipado â€” sin matching frÃ¡gil de cadenas.
Un ejecutor convertidor extrae el resultado estructurado; luego las aristas switch-case
enrutan a un manejador especializado para cada categorÃ­a.

Pipeline:
    Clasificador â†’ extract_category â†’ [Case: Question   â†’ handle_question ]
                                    â†’ [Case: Complaint  â†’ handle_complaint]
                                    â†’ [Default          â†’ handle_feedback ]

Contraste con workflow_conditional.py: las salidas estructuradas hacen que la lÃ³gica
de ramificaciÃ³n sea explÃ­cita, tipada y fÃ¡cil de extender.

Ejecutar:
    uv run examples/spanish/workflow_switch_case.py  (abre DevUI en http://localhost:8095)
"""

import asyncio
import os
import sys
from typing import Any, Literal

from agent_framework import Agent, AgentExecutorResponse, Case, Default, WorkflowBuilder, WorkflowContext, executor
from agent_framework.openai import OpenAIChatClient
from azure.identity.aio import DefaultAzureCredential, get_bearer_token_provider
from dotenv import load_dotenv
from pydantic import BaseModel
from typing_extensions import Never

load_dotenv(override=True)
API_HOST = os.getenv("API_HOST", "github")

# Configura el cliente de chat segÃºn el proveedor de API
async_credential = None
if API_HOST == "azure":
    async_credential = DefaultAzureCredential()
    token_provider = get_bearer_token_provider(async_credential, "https://cognitiveservices.azure.com/.default")
    client = OpenAIChatClient(
        base_url=f"{os.environ['AZURE_OPENAI_ENDPOINT']}/openai/v1/",
        api_key=token_provider,
        model_id=os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT"],
    )
elif API_HOST == "github":
    client = OpenAIChatClient(
        base_url="https://models.github.ai/inference",
        api_key=os.environ["GITHUB_TOKEN"],
        model_id=os.getenv("GITHUB_MODEL", "openai/gpt-5-mini"),
    )
else:
    client = OpenAIChatClient(
        api_key=os.environ["OPENAI_API_KEY"], model_id=os.environ.get("OPENAI_MODEL", "gpt-5-mini")
    )


# Modelo Pydantic usado como response_format â€” el LLM debe devolver JSON vÃ¡lido
# que coincida con este esquema, garantizando que la categorÃ­a siempre sea uno de los tres literales.
class ClassifyResult(BaseModel):
    """Resultado de clasificaciÃ³n estructurado del agente Clasificador."""

    category: Literal["Question", "Complaint", "Feedback"]
    original_message: str
    reasoning: str


# El agente clasificador usa response_format= para asegurar una salida estructurada.
# Esto es mÃ¡s confiable que pedirle al modelo que comience con una seÃ±al de texto.
classifier = Agent(
    client=client,
    name="Clasificador",
    instructions=(
        "Eres un clasificador de mensajes de clientes. "
        "Clasifica el mensaje entrante del cliente en exactamente una categorÃ­a: "
        "Question, Complaint o Feedback. "
        "Devuelve un objeto JSON con category, original_message y reasoning."
    ),
    response_format=ClassifyResult,
)


# Ejecutor convertidor: analiza la respuesta JSON del agente en un ClassifyResult tipado
# y lo reenvÃ­a. Las condiciones switch-case inspeccionarÃ¡n este objeto tipado.
@executor(id="extract_category")
async def extract_category(response: AgentExecutorResponse, ctx: WorkflowContext[ClassifyResult]) -> None:
    """Analiza la salida JSON estructurada del clasificador y la envÃ­a aguas abajo."""
    result = ClassifyResult.model_validate_json(response.agent_response.text)
    print(f"â†’ Clasificado como: {result.category} â€” {result.reasoning}")
    await ctx.send_message(result)


# Funciones de condiciÃ³n para el enrutamiento switch-case.
# Cada una recibe el ClassifyResult enviado por extract_category.
def is_question(msg: Any) -> bool:
    return isinstance(msg, ClassifyResult) and msg.category == "Question"


def is_complaint(msg: Any) -> bool:
    return isinstance(msg, ClassifyResult) and msg.category == "Complaint"


# Ejecutores terminales de manejo â€” uno por rama.
# Cada uno recibe el ClassifyResult y entrega una cadena de respuesta formateada.
@executor(id="handle_question")
async def handle_question(result: ClassifyResult, ctx: WorkflowContext[Never, str]) -> None:
    """Enruta una pregunta al equipo de Q&A."""
    await ctx.yield_output(
        f"â“ Pregunta enrutada al equipo de Q&A\n\n"
        f"Mensaje: {result.original_message}\n"
        f"Motivo: {result.reasoning}"
    )


@executor(id="handle_complaint")
async def handle_complaint(result: ClassifyResult, ctx: WorkflowContext[Never, str]) -> None:
    """Escala una queja al equipo de soporte."""
    await ctx.yield_output(
        f"âš ï¸  Queja escalada al equipo de soporte\n\n"
        f"Mensaje: {result.original_message}\n"
        f"Motivo: {result.reasoning}"
    )


@executor(id="handle_feedback")
async def handle_feedback(result: ClassifyResult, ctx: WorkflowContext[Never, str]) -> None:
    """ReenvÃ­a el feedback al equipo de producto."""
    await ctx.yield_output(
        f"ðŸ’¬ Feedback reenviado al equipo de producto\n\n"
        f"Mensaje: {result.original_message}\n"
        f"Motivo: {result.reasoning}"
    )


# Construye el workflow.
# add_switch_case_edge_group evalÃºa los casos en orden y toma el primero que coincida.
# Default captura todo lo que no coincida con un Case explÃ­cito.
workflow = (
    WorkflowBuilder(start_executor=classifier)
    .add_edge(classifier, extract_category)
    .add_switch_case_edge_group(
        extract_category,
        [
            Case(condition=is_question, target=handle_question),
            Case(condition=is_complaint, target=handle_complaint),
            Default(target=handle_feedback),
        ],
    )
    .build()
)


async def main():
    message = "Â¿CÃ³mo puedo restablecer mi contraseÃ±a?"
    print(f"Mensaje del cliente: {message}\n")
    events = await workflow.run(message)
    for output in events.get_outputs():
        print(output)

    if async_credential:
        await async_credential.close()


if __name__ == "__main__":
    if "--devui" in sys.argv:
        from agent_framework.devui import serve

        serve(entities=[workflow], port=8095, auto_open=True)
    else:
        asyncio.run(main())
