"""
Recuperaci√≥n de conocimiento (RAG) con PostgreSQL y b√∫squeda h√≠brida (vector + texto completo).

Diagrama:

 Entrada ‚îÄ‚îÄ‚ñ∂ Agente ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ LLM ‚îÄ‚îÄ‚ñ∂ Respuesta
               ‚îÇ                        ‚ñ≤
               ‚îÇ  buscar con entrada    ‚îÇ conocimiento relevante
               ‚ñº                        ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
         ‚îÇ   Base de   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ conocimiento‚îÇ
         ‚îÇ (Postgres)  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Este ejemplo usa pgvector para b√∫squeda por similitud vectorial y el
tsvector nativo de PostgreSQL para b√∫squeda de texto completo, combin√°ndolos
con Reciprocal Rank Fusion (RRF) para recuperaci√≥n h√≠brida. El agente busca
en la base de conocimiento *antes* de consultar al LLM ‚Äî sin necesidad de
llamar a una herramienta.

Requisitos:
  - PostgreSQL con extensi√≥n pgvector (ver docker-compose.yml)
  - Un modelo de embeddings (GitHub Models, Azure OpenAI u OpenAI)

Ver tambi√©n: agent_knowledge_sqlite.py para una versi√≥n m√°s simple solo con SQLite (b√∫squeda por palabras clave).
"""

import asyncio
import logging
import os
import sys
from typing import Any

import psycopg
from openai import OpenAI
from pgvector.psycopg import register_vector

from agent_framework import Agent, AgentSession, BaseContextProvider, Message, SessionContext, SupportsAgentRun
from agent_framework.openai import OpenAIChatClient
from azure.identity import DefaultAzureCredential as SyncDefaultAzureCredential
from azure.identity import get_bearer_token_provider as sync_get_bearer_token_provider
from azure.identity.aio import DefaultAzureCredential, get_bearer_token_provider
from dotenv import load_dotenv
from rich import print
from rich.logging import RichHandler

# ‚îÄ‚îÄ Logging ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
handler = RichHandler(show_path=False, rich_tracebacks=True, show_level=False)
logging.basicConfig(level=logging.WARNING, handlers=[handler], force=True, format="%(message)s")
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ‚îÄ‚îÄ Clientes OpenAI (chat + embeddings) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
load_dotenv(override=True)
API_HOST = os.getenv("API_HOST", "github")
POSTGRES_URL = os.getenv("POSTGRES_URL", "postgresql://admin:LocalPasswordOnly@db:5432/postgres")
EMBEDDING_DIMENSIONS = 256  # Dimensi√≥n reducida para eficiencia

async_credential = None
if API_HOST == "azure":
    # Credencial as√≠ncrona para el cliente de chat del framework
    async_credential = DefaultAzureCredential()
    async_token_provider = get_bearer_token_provider(async_credential, "https://cognitiveservices.azure.com/.default")
    # Credencial s√≠ncrona para el cliente de embeddings (SDK de OpenAI)
    sync_credential = SyncDefaultAzureCredential()
    sync_token_provider = sync_get_bearer_token_provider(sync_credential, "https://cognitiveservices.azure.com/.default")
    chat_client = OpenAIChatClient(
        base_url=f"{os.environ['AZURE_OPENAI_ENDPOINT']}/openai/v1/",
        api_key=async_token_provider,
        model_id=os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT"],
    )
    embed_client = OpenAI(
        base_url=f"{os.environ['AZURE_OPENAI_ENDPOINT']}/openai/v1/",
        api_key=sync_token_provider(),
    )
    embed_model = os.environ.get("AZURE_OPENAI_EMBEDDING_DEPLOYMENT", "text-embedding-3-small")
elif API_HOST == "github":
    chat_client = OpenAIChatClient(
        base_url="https://models.github.ai/inference",
        api_key=os.environ["GITHUB_TOKEN"],
        model_id=os.getenv("GITHUB_MODEL", "openai/gpt-4.1-mini"),
    )
    embed_client = OpenAI(
        base_url="https://models.github.ai/inference",
        api_key=os.environ["GITHUB_TOKEN"],
    )
    embed_model = "text-embedding-3-small"
else:
    chat_client = OpenAIChatClient(
        api_key=os.environ["OPENAI_API_KEY"], model_id=os.environ.get("OPENAI_MODEL", "gpt-4.1-mini")
    )
    embed_client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
    embed_model = "text-embedding-3-small"


def get_embedding(text: str) -> list[float]:
    """Obtiene un vector de embedding para el texto dado."""
    response = embed_client.embeddings.create(input=text, model=embed_model, dimensions=EMBEDDING_DIMENSIONS)
    return response.data[0].embedding


# ‚îÄ‚îÄ Base de conocimiento (PostgreSQL + pgvector) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

PRODUCTS = [
    {
        "name": "Botas de Senderismo TrailBlaze",
        "category": "Calzado",
        "price": 149.99,
        "description": (
            "Botas de senderismo impermeables con suelas Vibram, soporte de tobillo "
            "y forro transpirable Gore-Tex. Ideales para senderos rocosos y condiciones h√∫medas."
        ),
    },
    {
        "name": "Mochila SummitPack 40L",
        "category": "Mochilas",
        "price": 89.95,
        "description": (
            "Mochila ligera de 40 litros con compartimento para hidrataci√≥n, cubierta de lluvia "
            "y cintur√≥n de cadera ergon√≥mico. Perfecta para excursiones de un d√≠a o con pernocta."
        ),
    },
    {
        "name": "Chaqueta de Plum√≥n ArcticShield",
        "category": "Ropa",
        "price": 199.00,
        "description": (
            "Chaqueta de plum√≥n de ganso 800-fill con clasificaci√≥n de -28¬∞C. "
            "Incluye carcasa resistente al agua, dise√±o comprimible y capucha ajustable."
        ),
    },
    {
        "name": "Remo para Kayak RiverRun",
        "category": "Deportes Acu√°ticos",
        "price": 74.50,
        "description": (
            "Remo de fibra de vidrio para kayak con f√©rula ajustable y anillos antigoteo. "
            "Ligero (795 g), apto para kayak recreativo y de traves√≠a."
        ),
    },
    {
        "name": "Bastones de Trekking TerraFirm",
        "category": "Accesorios",
        "price": 59.99,
        "description": (
            "Bastones de trekking plegables de fibra de carbono con empu√±aduras de corcho y puntas de tungsteno. "
            "Ajustables de 60 a 137 cm, con amortiguaci√≥n anti-vibraci√≥n."
        ),
    },
    {
        "name": "Binoculares ClearView 10x42",
        "category": "√ìptica",
        "price": 129.00,
        "description": (
            "Binoculares de prisma de techo con aumento 10x y lentes objetivos de 42 mm. "
            "Cargados con nitr√≥geno y resistentes al agua. Ideales para observaci√≥n de aves y fauna."
        ),
    },
    {
        "name": "Linterna Frontal LED NightGlow",
        "category": "Iluminaci√≥n",
        "price": 34.99,
        "description": (
            "Linterna frontal recargable de 350 l√∫menes con modo de luz roja y haz ajustable. "
            "Clasificaci√≥n IPX6 de resistencia al agua, hasta 40 horas en modo bajo."
        ),
    },
    {
        "name": "Saco de Dormir CozyNest",
        "category": "Camping",
        "price": 109.00,
        "description": (
            "Saco de dormir tipo momia para tres estaciones, con clasificaci√≥n de -6¬∞C. "
            "Aislamiento sint√©tico, saco de compresi√≥n incluido. Pesa 1.1 kg."
        ),
    },
]


def create_knowledge_db(conn: psycopg.Connection) -> None:
    """Crea el cat√°logo de productos en PostgreSQL con pgvector e √≠ndices de texto completo."""
    conn.execute("CREATE EXTENSION IF NOT EXISTS vector")
    register_vector(conn)

    conn.execute("DROP TABLE IF EXISTS products")
    conn.execute(
        f"""
        CREATE TABLE products (
            id          SERIAL PRIMARY KEY,
            name        TEXT NOT NULL,
            category    TEXT NOT NULL,
            price       REAL NOT NULL,
            description TEXT NOT NULL,
            embedding   vector({EMBEDDING_DIMENSIONS})
        )
        """
    )
    # √çndice GIN para b√∫squeda de texto completo sobre nombre + descripci√≥n
    conn.execute(
        "CREATE INDEX ON products USING GIN (to_tsvector('spanish', name || ' ' || description))"
    )

    logger.info("[üìö Conocimiento] Generando embeddings para %d productos...", len(PRODUCTS))
    for product in PRODUCTS:
        text_for_embedding = f"{product['name']} - {product['category']}: {product['description']}"
        embedding = get_embedding(text_for_embedding)
        conn.execute(
            "INSERT INTO products (name, category, price, description, embedding) VALUES (%s, %s, %s, %s, %s)",
            (product["name"], product["category"], product["price"], product["description"], embedding),
        )

    conn.commit()
    logger.info("[üìö Conocimiento] Cat√°logo de productos cargado con embeddings.")


# ‚îÄ‚îÄ Proveedor de contexto personalizado para recuperaci√≥n h√≠brida ‚îÄ‚îÄ‚îÄ‚îÄ

# SQL de b√∫squeda h√≠brida usando Reciprocal Rank Fusion (RRF)
# Combina resultados de similitud vectorial y b√∫squeda de texto completo
HYBRID_SEARCH_SQL = f"""
WITH semantic_search AS (
    SELECT id, RANK() OVER (ORDER BY embedding <=> %(embedding)s::vector({EMBEDDING_DIMENSIONS})) AS rank
    FROM products
    ORDER BY embedding <=> %(embedding)s::vector({EMBEDDING_DIMENSIONS})
    LIMIT 20
),
keyword_search AS (
    SELECT id, RANK() OVER (ORDER BY ts_rank_cd(to_tsvector('spanish', name || ' ' || description), query) DESC)
    FROM products, plainto_tsquery('spanish', %(query)s) query
    WHERE to_tsvector('spanish', name || ' ' || description) @@ query
    ORDER BY ts_rank_cd(to_tsvector('spanish', name || ' ' || description), query) DESC
    LIMIT 20
)
SELECT
    COALESCE(semantic_search.id, keyword_search.id) AS id,
    COALESCE(1.0 / (%(k)s + semantic_search.rank), 0.0) +
    COALESCE(1.0 / (%(k)s + keyword_search.rank), 0.0) AS score
FROM semantic_search
FULL OUTER JOIN keyword_search ON semantic_search.id = keyword_search.id
ORDER BY score DESC
LIMIT %(limit)s
"""


class PostgresKnowledgeProvider(BaseContextProvider):
    """Recupera conocimiento relevante mediante b√∫squeda h√≠brida (vector + texto completo) con RRF.

    Usa pgvector para similitud sem√°ntica y tsvector de PostgreSQL para
    coincidencia por palabras clave, combinando resultados con Reciprocal
    Rank Fusion (RRF). Esto da mejor recuperaci√≥n que cualquier m√©todo solo.
    """

    def __init__(self, conn: psycopg.Connection, max_results: int = 3):
        super().__init__(source_id="postgres-knowledge")
        self.conn = conn
        self.max_results = max_results

    def _search(self, query: str) -> list[dict]:
        """Ejecuta b√∫squeda h√≠brida (vector + texto completo) y devuelve productos coincidentes."""
        query_embedding = get_embedding(query)

        cursor = self.conn.execute(
            HYBRID_SEARCH_SQL,
            {"embedding": query_embedding, "query": query, "k": 60, "limit": self.max_results},
        )
        result_ids = [row[0] for row in cursor.fetchall()]
        if not result_ids:
            return []

        # Obtener detalles completos de los productos encontrados
        products = []
        for product_id in result_ids:
            row = self.conn.execute(
                "SELECT name, category, price, description FROM products WHERE id = %s",
                (product_id,),
            ).fetchone()
            if row:
                products.append({"name": row[0], "category": row[1], "price": row[2], "description": row[3]})
        return products

    def _format_results(self, results: list[dict]) -> str:
        """Formatea los resultados de b√∫squeda como texto para el contexto del LLM."""
        lines = ["Informaci√≥n relevante de productos de nuestro cat√°logo:\n"]
        for product in results:
            lines.append(
                f"- **{product['name']}** ({product['category']}, ${product['price']:.2f}): "
                f"{product['description']}"
            )
        return "\n".join(lines)

    async def before_run(
        self,
        *,
        agent: SupportsAgentRun,
        session: AgentSession,
        context: SessionContext,
        state: dict[str, Any],
    ) -> None:
        """Busca en la base de conocimiento con el √∫ltimo mensaje del usuario e inyecta resultados."""
        user_text = next((msg.text for msg in reversed(context.input_messages) if msg.role == "user" and msg.text), None)
        if not user_text:
            return

        results = self._search(user_text)
        if not results:
            logger.info("[üìö Conocimiento] No se encontraron productos para: %s", user_text)
            return

        logger.info("[üìö Conocimiento] Se encontraron %d producto(s) para: %s", len(results), user_text)

        context.extend_messages(
            self.source_id,
            [Message(role="user", text=self._format_results(results))],
        )


# ‚îÄ‚îÄ Configuraci√≥n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def setup_db() -> psycopg.Connection:
    """Conecta a PostgreSQL y carga la base de conocimiento."""
    conn = psycopg.connect(POSTGRES_URL)
    create_knowledge_db(conn)
    return conn


conn = setup_db()
knowledge_provider = PostgresKnowledgeProvider(conn=conn)

agent = Agent(
    client=chat_client,
    instructions=(
        "Eres un asistente de compras de equipo para actividades al aire libre de la tienda 'TrailBuddy'. "
        "Responde las preguntas del cliente usando SOLO la informaci√≥n de productos proporcionada en el contexto. "
        "Si no se encuentran productos relevantes en el contexto, di que no tienes informaci√≥n sobre ese art√≠culo. "
        "Incluye precios al recomendar productos."
    ),
    context_providers=[knowledge_provider],
)


async def main() -> None:
    """Demuestra b√∫squeda h√≠brida RAG con varias consultas."""
    print("\n[bold]=== Recuperaci√≥n de Conocimiento (RAG) con B√∫squeda H√≠brida en PostgreSQL ===[/bold]")

    # Consulta 1: Deber√≠a encontrar botas de senderismo y bastones de trekking
    print("[blue]Usuario:[/blue] Estoy planeando una excursi√≥n. ¬øQu√© botas y bastones me recomiendan?")
    response = await agent.run("Estoy planeando una excursi√≥n. ¬øQu√© botas y bastones me recomiendan?")
    print(f"[green]Agente:[/green] {response.text}\n")

    # Consulta 2: Coincidencia sem√°ntica ‚Äî "art√≠culos para observar fauna" ‚Üí binoculares
    print("[blue]Usuario:[/blue] Quiero art√≠culos para observar fauna silvestre")
    response = await agent.run("Quiero art√≠culos para observar fauna silvestre")
    print(f"[green]Agente:[/green] {response.text}\n")

    conn.close()

    if async_credential:
        await async_credential.close()


if __name__ == "__main__":
    if "--devui" in sys.argv:
        from agent_framework.devui import serve

        serve(entities=[agent], auto_open=True)
    else:
        asyncio.run(main())
